{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Фреймворк PyTorch для разработки искусственных нейронных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Урок 5. Face Detection and Emotion Recognition (Курсовая работа)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно написать приложение, которое будет считывать и выводить кадры с веб-камеры. В процессе считывания определять что перед камерой находится человек, задетектировав его лицо на кадре. После этого, человек показывает жесты руками, а алгоритм должен считать их и определенным образом реагировать на эти жесты.\n",
    "На то, как система будет реагировать на определенные жесты - выбор за вами. Например, на определенный жест (жест пис), система будет здороваться с человеком. На другой, будет делать скриншот экрана. И т.д.\n",
    "Для распознавания жестов, вам надо будет скачать датасет https://www.kaggle.com/gti-upm/leapgestrecog, разработать модель для обучения и обучить эту модель.\n",
    "\n",
    "*(Усложненное задание) Все тоже самое, но воспользоваться этим датасетом:\n",
    "https://fitnessallyapp.com/datasets/jester/v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 2. Использование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель обученная на датасете leapgestrecog получилась плохой. Пробовал разные модели и аугментацию, но всё-равно ничего хорошего не выходило. Думаю, это из-за того, что жесты, на которых проходит обучение, далеки от реальности. Поэтому решил сделать обучение модели на основе cv2 в режиме онлайн. Получилось гораздо лучше. Ссылка на ноутбук с моделью: https://github.com/SemenovR/GB_PyTorch/blob/lesson_5/lesson_5/lesson_5_run_train.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as tt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # Для обхода бага библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, size_n, size_p, debug):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.debug = debug\n",
    "        \n",
    "        size_pre = in_channels\n",
    "        size_cur = size_n\n",
    "        size_c = size_p / 2\n",
    "        self.conv1 = self.conv_block(size_pre, size_cur)\n",
    "        self.conv2 = self.conv_block(size_cur, size_cur, pool=True)\n",
    "        self.drop11 = nn.Dropout(0.25)\n",
    "        self.res1 = nn.Sequential(self.conv_block(size_cur, size_cur), self.conv_block(size_cur, size_cur))\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        \n",
    "        size_pre = size_cur\n",
    "        size_cur = size_cur * 2\n",
    "        size_c = size_c / 2\n",
    "        self.conv3 = self.conv_block(size_pre, size_cur)\n",
    "        self.conv4 = self.conv_block(size_cur, size_cur, pool=True)\n",
    "        self.drop12 = nn.Dropout(0.25)\n",
    "        self.res2 = nn.Sequential(self.conv_block(size_cur, size_cur), self.conv_block(size_cur, size_cur))\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        \n",
    "        size_pre = size_cur\n",
    "        size_cur = size_cur * 2\n",
    "        size_c = size_c / 2\n",
    "        self.conv5 = self.conv_block(size_pre, size_cur)\n",
    "        self.conv6 = self.conv_block(size_cur, size_cur, pool=True)\n",
    "        self.drop13 = nn.Dropout(0.25)\n",
    "        self.res3 = nn.Sequential(self.conv_block(size_cur, size_cur), self.conv_block(size_cur, size_cur))\n",
    "        self.drop3 = nn.Dropout(0.5)\n",
    "\n",
    "        #size_pre = size_cur\n",
    "        #size_cur = size_cur * 2\n",
    "        #size_c = size_c / 2\n",
    "        #self.conv7 = self.conv_block(size_pre, size_cur)\n",
    "        #self.conv8 = self.conv_block(size_cur, size_cur, pool=True)\n",
    "        #self.drop14 = nn.Dropout(0.25)\n",
    "        #self.res4 = nn.Sequential(self.conv_block(size_cur, size_cur), self.conv_block(size_cur, size_cur))\n",
    "        #self.drop4 = nn.Dropout(0.5)\n",
    "        \n",
    "        size_k = 6\n",
    "        size_f = int((math.floor((size_c - size_k) / size_k) + 1)**2 * size_cur)\n",
    "        self.maxp = nn.MaxPool2d(size_k)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.line = nn.Linear(size_f, num_classes)\n",
    "        \n",
    "    @staticmethod\n",
    "    def conv_block(in_channels, out_channels, pool=False):\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "                  nn.BatchNorm2d(out_channels), \n",
    "                  nn.ELU(inplace=True)]\n",
    "        if pool: layers.append(nn.MaxPool2d(2))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "\n",
    "    def call_layer(self, data, layer):\n",
    "        if self.debug == True:\n",
    "            print(self.counter, data.shape)\n",
    "            self.counter += 1\n",
    "        return layer(data)\n",
    "            \n",
    "        \n",
    "    def forward(self, xb):\n",
    "        self.counter = 0\n",
    "        \n",
    "        out = self.call_layer(xb, self.conv1)\n",
    "        out = self.call_layer(out, self.conv2)\n",
    "        out = self.call_layer(out, self.drop11)\n",
    "        out = self.call_layer(out, self.res1) + out\n",
    "        out = self.call_layer(out, self.drop1)\n",
    "        \n",
    "        out = self.call_layer(out, self.conv3)\n",
    "        out = self.call_layer(out, self.conv4)\n",
    "        out = self.call_layer(out, self.drop12)\n",
    "        out = self.call_layer(out, self.res2) + out\n",
    "        out = self.call_layer(out, self.drop2)\n",
    "        \n",
    "        out = self.call_layer(out, self.conv5)\n",
    "        out = self.call_layer(out, self.conv6)\n",
    "        out = self.call_layer(out, self.drop13)\n",
    "        out = self.call_layer(out, self.res3) + out\n",
    "        out = self.call_layer(out, self.drop3)\n",
    "\n",
    "        #out = self.call_layer(out, self.conv7)\n",
    "        #out = self.call_layer(out, self.conv8)\n",
    "        #out = self.call_layer(out, self.drop14)\n",
    "        #out = self.call_layer(out, self.res4) + out\n",
    "        #out = self.call_layer(out, self.drop4)\n",
    "        \n",
    "        out = self.call_layer(out, self.maxp)\n",
    "        out = self.call_layer(out, self.flat)\n",
    "        out = self.call_layer(out, self.line)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс детектирования и обработки жестов с веб-камеры \n",
    "class GestureDetector(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.channels = 1\n",
    "        self.classes = 10\n",
    "        self.size_n = 256\n",
    "        self.size_p = 128  \n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = ResNet(self.channels, self.classes, self.size_n, self.size_p, False).to(self.device)\n",
    "        \n",
    "        self.model.load_state_dict(torch.load('./models/model1.pth'))\n",
    "        self.model.eval()\n",
    "\n",
    "    # Функция рисования найденных параметров на кадре\n",
    "    def _draw(self, frame, text):\n",
    "        try:\n",
    "            # пишем на кадре какой жест распознан\n",
    "            cv2.putText(frame, text, (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        except:\n",
    "            print('Something wrong im draw function!')\n",
    "\n",
    "        return frame\n",
    "    \n",
    "    @staticmethod\n",
    "    def digit_to_classname(digit):\n",
    "        if digit == 0:\n",
    "            return 'palm'\n",
    "        elif digit == 1:\n",
    "            return 'I'\n",
    "        elif digit == 2:\n",
    "            return 'fist'\n",
    "        elif digit == 3:\n",
    "            return 'fist_moved'\n",
    "        elif digit == 4:\n",
    "            return 'thumb'\n",
    "        elif digit == 5:\n",
    "            return 'index'\n",
    "        elif digit == 6:\n",
    "            return 'ok'\n",
    "        elif digit == 7:\n",
    "            return 'palm_moved'\n",
    "        elif digit == 8:\n",
    "            return 'c'\n",
    "        elif digit == 9:\n",
    "            return 'down'\n",
    "\n",
    "    # Функция в которой будет происходить процесс считывания и обработки каждого кадра\n",
    "    def run(self):              \n",
    "        transforms = tt.Compose([# Настройки для расширения датасета\n",
    "                     tt.Grayscale(num_output_channels=1), # Картинки чернобелые\n",
    "                     tt.Resize((self.size_p, self.size_p), interpolation=Image.NEAREST),\n",
    "                     tt.ToTensor(),\n",
    "                     tt.Normalize((0.1307,), (0.3081,)),\n",
    "        ])                      # Приведение к тензору\n",
    "\n",
    "        # Заходим в бесконечный цикл\n",
    "        while True:\n",
    "            # Считываем каждый новый кадр - frame\n",
    "            # ret - логическая переменая. Смысл - считали ли мы кадр с потока или нет\n",
    "            ret, frame = cap.read()\n",
    "            #try:\n",
    "\n",
    "            data = Variable(transforms(Image.fromarray(frame))).unsqueeze(0)\n",
    "            data = data.to(self.device)\n",
    "            \n",
    "            predict = self.model(data)\n",
    "            predict_text = self.digit_to_classname(predict.argmax())\n",
    "            self._draw(frame, predict_text)\n",
    "\n",
    "            #except:\n",
    "            #    print('Something wrong im main cycle!')\n",
    "\n",
    "            # Показываем кадр в окне, и называем его(окно) - 'Gesture Detection'\n",
    "            cv2.imshow('Gesture Detection', frame)\n",
    "            \n",
    "            # Функция, которая проверяет нажатие на клавишу 'q'\n",
    "            # Если нажатие произошло - выход из цикла. Конец работы приложения\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "        # Очищаем все объекты opencv, что мы создали\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        \n",
    "# Создаем объект для считывания потока с веб-камеры(обычно вебкамера идет под номером 0. иногда 1)\n",
    "cap = cv2.VideoCapture(0)  \n",
    "# Создаем объект нашего класса приложения\n",
    "fcd = GestureDetector()\n",
    "# Запускаем\n",
    "fcd.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
